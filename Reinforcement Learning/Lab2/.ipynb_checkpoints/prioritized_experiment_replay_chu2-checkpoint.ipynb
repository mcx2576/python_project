{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "v3Qx8k1-0x_y",
    "outputId": "cae12f2a-75c4-429a-fd34-d9e4b22bb11e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Using cached https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /home/cecilia/miniconda3/envs/rl2018/lib/python3.6/site-packages (from torchvision) (1.15.2)\n",
      "Requirement already satisfied: torch in /home/cecilia/miniconda3/envs/rl2018/lib/python3.6/site-packages (from torchvision) (0.4.1)\n",
      "Requirement already satisfied: six in /home/cecilia/miniconda3/envs/rl2018/lib/python3.6/site-packages (from torchvision) (1.11.0)\n",
      "Collecting pillow>=4.1.1 (from torchvision)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.0MB 7.6MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pillow, torchvision\n",
      "Successfully installed pillow-5.3.0 torchvision-0.2.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install gym\n",
    "#!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_oejd3tzsJU"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "from collections import deque\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRNgoigAzVOD"
   },
   "outputs": [],
   "source": [
    "# SumTree\n",
    "# a binary tree data structure where the parent’s value is the sum of its children\n",
    "torch.manual_seed(123)\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.n_entries = 0\n",
    "\n",
    "    # update to the root node\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # find sample on leaf node\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    # store priority and sample\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    # update priority\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    # get priority and sample\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44ItaPopyyWJ"
   },
   "outputs": [],
   "source": [
    "class Memory:  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.01\n",
    "    #a = 0.6\n",
    "    #beta = 0.4\n",
    "\n",
    "    beta_increment_per_sampling = 0.000025\n",
    "\n",
    "    def __init__(self, capacity, a, beta):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "        self.a = a\n",
    "        self.beta = beta\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / n\n",
    "        priorities = []\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max()\n",
    "\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1526
    },
    "colab_type": "code",
    "id": "6i-uE15uzf_E",
    "outputId": "567b5b6d-3e9d-4a9c-8c9c-f5c049800708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cecilia/miniconda3/envs/rl2018/lib/python3.6/site-packages/ipykernel_launcher.py:64: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.47348514 -0.00668942]]\n",
      "episode: 0   score: -200.0   memory length: 200   epsilon: 1  beta 0.0001\n",
      "[[-0.42461862  0.00988408]]\n",
      "episode: 1   score: -200.0   memory length: 400   epsilon: 1  beta 0.0001\n",
      "[[-0.66470931  0.0064344 ]]\n",
      "episode: 2   score: -200.0   memory length: 600   epsilon: 1  beta 0.0001\n",
      "[[-0.78751868 -0.00281763]]\n",
      "episode: 3   score: -200.0   memory length: 800   epsilon: 1  beta 0.0001\n",
      "[[-0.41740868 -0.00886999]]\n",
      "episode: 4   score: -200.0   memory length: 1000   epsilon: 0.99905  beta 0.000125\n",
      "[[-0.46790557  0.00859812]]\n",
      "episode: 5   score: -200.0   memory length: 1200   epsilon: 0.8090499999999987  beta 0.005124999999999983\n",
      "[[-0.61982024 -0.00738196]]\n",
      "episode: 6   score: -200.0   memory length: 1400   epsilon: 0.6190499999999974  beta 0.010125000000000033\n",
      "[[-0.76594592  0.01228283]]\n",
      "episode: 7   score: -200.0   memory length: 1600   epsilon: 0.42904999999999616  beta 0.015125000000000176\n",
      "[[-0.46872643  0.00351455]]\n",
      "episode: 8   score: -200.0   memory length: 1800   epsilon: 0.23904999999999488  beta 0.02012500000000032\n",
      "[[-0.3384944   0.00422232]]\n",
      "episode: 9   score: -200.0   memory length: 2000   epsilon: 0.0499999999999937  beta 0.025125000000000463\n",
      "[[-0.56794818 -0.00552903]]\n",
      "episode: 10   score: -200.0   memory length: 2200   epsilon: 0.0499999999999937  beta 0.030125000000000606\n",
      "[[-0.6753622   0.02191184]]\n",
      "episode: 11   score: -200.0   memory length: 2400   epsilon: 0.0499999999999937  beta 0.03512500000000021\n",
      "[[-0.4804419  0.0250647]]\n",
      "episode: 12   score: -200.0   memory length: 2600   epsilon: 0.0499999999999937  beta 0.04012499999999966\n",
      "[[-0.61601401 -0.03820492]]\n",
      "episode: 13   score: -200.0   memory length: 2800   epsilon: 0.0499999999999937  beta 0.04512499999999911\n",
      "[[-0.47102922  0.03576214]]\n",
      "episode: 14   score: -200.0   memory length: 3000   epsilon: 0.0499999999999937  beta 0.05012499999999856\n",
      "[[-0.68751927  0.00460259]]\n",
      "episode: 15   score: -200.0   memory length: 3200   epsilon: 0.0499999999999937  beta 0.05512499999999801\n",
      "[[-0.91872025 -0.00383454]]\n",
      "episode: 16   score: -200.0   memory length: 3400   epsilon: 0.0499999999999937  beta 0.06012499999999746\n",
      "[[-0.24576767 -0.00802645]]\n",
      "episode: 17   score: -200.0   memory length: 3600   epsilon: 0.0499999999999937  beta 0.06512499999999691\n",
      "[[-0.45933394 -0.0447218 ]]\n",
      "episode: 18   score: -200.0   memory length: 3800   epsilon: 0.0499999999999937  beta 0.07012499999999636\n",
      "[[-0.431598    0.03122024]]\n",
      "episode: 19   score: -200.0   memory length: 4000   epsilon: 0.0499999999999937  beta 0.0751249999999958\n",
      "[[-0.67923879  0.00930574]]\n",
      "episode: 20   score: -200.0   memory length: 4200   epsilon: 0.0499999999999937  beta 0.08012499999999526\n",
      "[[-0.25437882  0.00790955]]\n",
      "episode: 21   score: -200.0   memory length: 4400   epsilon: 0.0499999999999937  beta 0.0851249999999947\n",
      "[[-0.72946022 -0.03497645]]\n",
      "episode: 22   score: -200.0   memory length: 4600   epsilon: 0.0499999999999937  beta 0.09012499999999415\n",
      "[[-0.2850854  -0.04899139]]\n",
      "episode: 23   score: -200.0   memory length: 4800   epsilon: 0.0499999999999937  beta 0.0951249999999936\n",
      "[[-0.74483612 -0.04557684]]\n",
      "episode: 24   score: -200.0   memory length: 5000   epsilon: 0.0499999999999937  beta 0.10012499999999305\n",
      "[[0.5018936  0.03549401]]\n",
      "episode: 25   score: -137.0   memory length: 5137   epsilon: 0.0499999999999937  beta 0.10354999999999268\n",
      "[[-0.79690941  0.02827192]]\n",
      "episode: 26   score: -200.0   memory length: 5337   epsilon: 0.0499999999999937  beta 0.10854999999999213\n",
      "[[-1.19233427  0.00403079]]\n",
      "episode: 27   score: -200.0   memory length: 5537   epsilon: 0.0499999999999937  beta 0.11354999999999157\n",
      "[[0.15471379 0.03037428]]\n",
      "episode: 28   score: -200.0   memory length: 5737   epsilon: 0.0499999999999937  beta 0.11854999999999102\n",
      "[[-0.12014352  0.05649284]]\n",
      "episode: 29   score: -200.0   memory length: 5937   epsilon: 0.0499999999999937  beta 0.12354999999999047\n",
      "[[0.50220534 0.00795   ]]\n",
      "episode: 30   score: -163.0   memory length: 6100   epsilon: 0.0499999999999937  beta 0.12762499999999002\n",
      "[[-0.28451878 -0.05329229]]\n",
      "episode: 31   score: -200.0   memory length: 6300   epsilon: 0.0499999999999937  beta 0.13262499999998947\n",
      "[[0.01491067 0.05345597]]\n",
      "episode: 32   score: -200.0   memory length: 6500   epsilon: 0.0499999999999937  beta 0.13762499999998892\n",
      "[[0.07010118 0.05255575]]\n",
      "episode: 33   score: -200.0   memory length: 6700   epsilon: 0.0499999999999937  beta 0.14262499999998837\n",
      "[[0.51940193 0.04917299]]\n",
      "episode: 34   score: -144.0   memory length: 6844   epsilon: 0.0499999999999937  beta 0.14622499999998798\n",
      "[[0.53037591 0.03535742]]\n",
      "episode: 35   score: -151.0   memory length: 6995   epsilon: 0.0499999999999937  beta 0.14999999999998756\n",
      "[[0.51091278 0.02644299]]\n",
      "episode: 36   score: -177.0   memory length: 7172   epsilon: 0.0499999999999937  beta 0.15442499999998707\n",
      "[[0.50029508 0.04125021]]\n",
      "episode: 37   score: -157.0   memory length: 7329   epsilon: 0.0499999999999937  beta 0.15834999999998664\n",
      "[[0.53107093 0.04909766]]\n",
      "episode: 38   score: -149.0   memory length: 7478   epsilon: 0.0499999999999937  beta 0.16207499999998623\n",
      "[[0.52851705 0.04450698]]\n",
      "episode: 39   score: -157.0   memory length: 7635   epsilon: 0.0499999999999937  beta 0.1659999999999858\n",
      "[[0.50511091 0.03430229]]\n",
      "episode: 40   score: -153.0   memory length: 7788   epsilon: 0.0499999999999937  beta 0.16982499999998538\n",
      "[[0.5348578  0.04795707]]\n",
      "episode: 41   score: -164.0   memory length: 7952   epsilon: 0.0499999999999937  beta 0.17392499999998492\n",
      "[[0.53798537 0.03968334]]\n",
      "episode: 42   score: -153.0   memory length: 8105   epsilon: 0.0499999999999937  beta 0.1777499999999845\n",
      "[[0.5348578  0.04795707]]\n",
      "episode: 43   score: -194.0   memory length: 8299   epsilon: 0.0499999999999937  beta 0.18259999999998397\n",
      "[[0.5270755  0.04374392]]\n",
      "episode: 44   score: -153.0   memory length: 8452   epsilon: 0.0499999999999937  beta 0.18642499999998355\n",
      "[[0.50491589 0.04606464]]\n",
      "episode: 45   score: -138.0   memory length: 8590   epsilon: 0.0499999999999937  beta 0.18987499999998317\n",
      "[[0.53084289 0.04594217]]\n",
      "episode: 46   score: -150.0   memory length: 8740   epsilon: 0.0499999999999937  beta 0.19362499999998276\n",
      "[[0.53804841 0.04283357]]\n",
      "episode: 47   score: -144.0   memory length: 8884   epsilon: 0.0499999999999937  beta 0.19722499999998236\n",
      "[[0.52715302 0.03618143]]\n",
      "episode: 48   score: -133.0   memory length: 9017   epsilon: 0.0499999999999937  beta 0.200549999999982\n",
      "[[0.50141183 0.02480293]]\n",
      "episode: 49   score: -139.0   memory length: 9156   epsilon: 0.0499999999999937  beta 0.2040249999999816\n",
      "[[0.53514997 0.03757316]]\n",
      "episode: 50   score: -176.0   memory length: 9332   epsilon: 0.0499999999999937  beta 0.20842499999998113\n",
      "[[0.50510237 0.04398656]]\n",
      "episode: 51   score: -162.0   memory length: 9494   epsilon: 0.0499999999999937  beta 0.21247499999998068\n",
      "[[0.51359705 0.03581323]]\n",
      "episode: 52   score: -145.0   memory length: 9639   epsilon: 0.0499999999999937  beta 0.21609999999998028\n",
      "[[0.52734994 0.04057641]]\n",
      "episode: 53   score: -177.0   memory length: 9816   epsilon: 0.0499999999999937  beta 0.2205249999999798\n",
      "[[0.53084289 0.04594217]]\n",
      "episode: 54   score: -144.0   memory length: 9960   epsilon: 0.0499999999999937  beta 0.2241249999999794\n",
      "[[0.50424803 0.00676636]]\n",
      "episode: 55   score: -141.0   memory length: 10101   epsilon: 0.0499999999999937  beta 0.227649999999979\n",
      "[[0.52677059 0.04489819]]\n",
      "episode: 56   score: -143.0   memory length: 10244   epsilon: 0.0499999999999937  beta 0.23122499999997861\n",
      "[[0.50267856 0.04356744]]\n",
      "episode: 57   score: -144.0   memory length: 10388   epsilon: 0.0499999999999937  beta 0.23482499999997822\n",
      "[[0.53857962 0.04265288]]\n",
      "episode: 58   score: -139.0   memory length: 10527   epsilon: 0.0499999999999937  beta 0.23829999999997784\n",
      "[[0.52760708 0.03166843]]\n",
      "episode: 59   score: -144.0   memory length: 10671   epsilon: 0.0499999999999937  beta 0.24189999999997744\n",
      "[[0.513164   0.01547498]]\n",
      "episode: 60   score: -110.0   memory length: 10781   epsilon: 0.0499999999999937  beta 0.24464999999997714\n",
      "[[0.50599565 0.0137118 ]]\n",
      "episode: 61   score: -145.0   memory length: 10926   epsilon: 0.0499999999999937  beta 0.24827499999997674\n",
      "[[0.52997849 0.04233487]]\n",
      "episode: 62   score: -160.0   memory length: 11086   epsilon: 0.0499999999999937  beta 0.2522749999999763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50022556 0.03891899]]\n",
      "episode: 63   score: -179.0   memory length: 11265   epsilon: 0.0499999999999937  beta 0.25674999999997583\n",
      "[[0.50121046 0.04051901]]\n",
      "episode: 64   score: -141.0   memory length: 11406   epsilon: 0.0499999999999937  beta 0.26027499999997544\n",
      "[[0.54237467 0.04278245]]\n",
      "episode: 65   score: -146.0   memory length: 11552   epsilon: 0.0499999999999937  beta 0.26392499999997504\n",
      "[[0.52587942 0.04376877]]\n",
      "episode: 66   score: -158.0   memory length: 11710   epsilon: 0.0499999999999937  beta 0.2678749999999746\n",
      "[[0.52936026 0.03927924]]\n",
      "episode: 67   score: -187.0   memory length: 11897   epsilon: 0.0499999999999937  beta 0.2725499999999741\n",
      "[[0.50501464 0.0071409 ]]\n",
      "episode: 68   score: -148.0   memory length: 12045   epsilon: 0.0499999999999937  beta 0.2762499999999737\n",
      "[[-0.38453136 -0.06642014]]\n",
      "episode: 69   score: -200.0   memory length: 12245   epsilon: 0.0499999999999937  beta 0.28124999999997313\n",
      "[[0.51749783 0.0441935 ]]\n",
      "episode: 70   score: -149.0   memory length: 12394   epsilon: 0.0499999999999937  beta 0.2849749999999727\n",
      "[[0.50790423 0.04708823]]\n",
      "episode: 71   score: -143.0   memory length: 12537   epsilon: 0.0499999999999937  beta 0.28854999999997233\n",
      "[[-0.80033178  0.04828288]]\n",
      "episode: 72   score: -200.0   memory length: 12737   epsilon: 0.0499999999999937  beta 0.2935499999999718\n",
      "[[0.50597789 0.0108198 ]]\n",
      "episode: 73   score: -161.0   memory length: 12898   epsilon: 0.0499999999999937  beta 0.29757499999997133\n",
      "[[0.52246639 0.03434092]]\n",
      "episode: 74   score: -180.0   memory length: 13078   epsilon: 0.0499999999999937  beta 0.30207499999997084\n",
      "[[ 0.4138965  -0.01390802]]\n",
      "episode: 75   score: -200.0   memory length: 13278   epsilon: 0.0499999999999937  beta 0.3070749999999703\n",
      "[[0.50859555 0.02844023]]\n",
      "episode: 76   score: -141.0   memory length: 13419   epsilon: 0.0499999999999937  beta 0.3105999999999699\n",
      "[[0.50061981 0.02945408]]\n",
      "episode: 77   score: -127.0   memory length: 13546   epsilon: 0.0499999999999937  beta 0.31377499999996955\n",
      "[[0.53856933 0.0493285 ]]\n",
      "episode: 78   score: -145.0   memory length: 13691   epsilon: 0.0499999999999937  beta 0.31739999999996915\n",
      "[[0.50461634 0.04811517]]\n",
      "episode: 79   score: -173.0   memory length: 13864   epsilon: 0.0499999999999937  beta 0.3217249999999687\n",
      "[[0.50817076 0.04634922]]\n",
      "episode: 80   score: -184.0   memory length: 14048   epsilon: 0.0499999999999937  beta 0.32632499999996817\n",
      "[[0.5348578  0.04795707]]\n",
      "episode: 81   score: -182.0   memory length: 14230   epsilon: 0.0499999999999937  beta 0.33087499999996767\n",
      "[[0.51479542 0.01535293]]\n",
      "episode: 82   score: -153.0   memory length: 14383   epsilon: 0.0499999999999937  beta 0.33469999999996725\n",
      "[[-0.33676151  0.06195079]]\n",
      "episode: 83   score: -200.0   memory length: 14583   epsilon: 0.0499999999999937  beta 0.3396999999999667\n",
      "[[-0.25975338  0.05344282]]\n",
      "episode: 84   score: -200.0   memory length: 14783   epsilon: 0.0499999999999937  beta 0.34469999999996614\n",
      "[[0.50832469 0.02933754]]\n",
      "episode: 85   score: -141.0   memory length: 14924   epsilon: 0.0499999999999937  beta 0.34822499999996576\n",
      "[[-0.85303415 -0.06431952]]\n",
      "episode: 86   score: -200.0   memory length: 15124   epsilon: 0.0499999999999937  beta 0.3532249999999652\n",
      "[[-1.03023818  0.02781034]]\n",
      "episode: 87   score: -200.0   memory length: 15324   epsilon: 0.0499999999999937  beta 0.35822499999996466\n",
      "[[-0.09860247  0.0591046 ]]\n",
      "episode: 88   score: -200.0   memory length: 15524   epsilon: 0.0499999999999937  beta 0.3632249999999641\n",
      "[[-1.19026366  0.00649445]]\n",
      "episode: 89   score: -200.0   memory length: 15724   epsilon: 0.0499999999999937  beta 0.36822499999996355\n",
      "[[-0.27839065  0.02340703]]\n",
      "episode: 90   score: -200.0   memory length: 15924   epsilon: 0.0499999999999937  beta 0.373224999999963\n",
      "[[-0.93323194  0.03560543]]\n",
      "episode: 91   score: -200.0   memory length: 16124   epsilon: 0.0499999999999937  beta 0.37822499999996245\n",
      "[[0.52743972 0.03832134]]\n",
      "episode: 92   score: -144.0   memory length: 16268   epsilon: 0.0499999999999937  beta 0.38182499999996206\n",
      "[[0.50309229 0.03742687]]\n",
      "episode: 93   score: -151.0   memory length: 16419   epsilon: 0.0499999999999937  beta 0.38559999999996164\n",
      "[[-0.90182434  0.00641748]]\n",
      "episode: 94   score: -200.0   memory length: 16619   epsilon: 0.0499999999999937  beta 0.3905999999999611\n",
      "[[-0.63952211  0.05283448]]\n",
      "episode: 95   score: -200.0   memory length: 16819   epsilon: 0.0499999999999937  beta 0.39559999999996054\n",
      "[[0.07653457 0.0375663 ]]\n",
      "episode: 96   score: -200.0   memory length: 17019   epsilon: 0.0499999999999937  beta 0.40059999999996\n",
      "[[-0.38109191 -0.01774107]]\n",
      "episode: 97   score: -200.0   memory length: 17219   epsilon: 0.0499999999999937  beta 0.40559999999995944\n",
      "[[0.18291063 0.03425019]]\n",
      "episode: 98   score: -200.0   memory length: 17419   epsilon: 0.0499999999999937  beta 0.4105999999999589\n",
      "[[-0.39109699  0.01899338]]\n",
      "episode: 99   score: -200.0   memory length: 17619   epsilon: 0.0499999999999937  beta 0.41559999999995834\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPISODES = 100\n",
    "\n",
    "# approximate Q function using Neural Network\n",
    "# state is input and Q Value of each action is output of network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_size, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# DQN Agent for the Cartpole\n",
    "# it uses Neural Network to approximate q function\n",
    "# and prioritized experience replay memory & target q network\n",
    "class DQNAgent():\n",
    "    def __init__(self, state_size, action_size, a, beta):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = True\n",
    "        self.load_model = False\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.8\n",
    "        self.learning_rate = 0.001\n",
    "        self.memory_size = 20000\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.05\n",
    "        self.explore_step = 1000\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / self.explore_step\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "\n",
    "        # create prioritized replay memory using SumTree\n",
    "        self.memory = Memory(self.memory_size, a, beta)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.model.apply(self.weights_init)\n",
    "        self.target_model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),\n",
    "                                    lr=self.learning_rate)\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model = torch.load('save_model/cartpole_dqn')\n",
    "\n",
    "    # weight xavier initialize\n",
    "    def weights_init(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Linear') != -1:\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            state = torch.from_numpy(state)\n",
    "            state = Variable(state).float().cpu()\n",
    "            q_value = self.model(state)\n",
    "            _, action = torch.max(q_value, 1)\n",
    "            return int(action)\n",
    "\n",
    "    # save sample (error,<s,a,r,s'>) to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        target = self.model(Variable(torch.FloatTensor(state))).data\n",
    "        old_val = target[0][action]\n",
    "        target_val = self.target_model(Variable(torch.FloatTensor(next_state))).data\n",
    "        if done:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + self.discount_factor * torch.max(target_val)\n",
    "\n",
    "        error = abs(old_val - target[0][action])\n",
    "\n",
    "        self.memory.add(error, (state, action, reward, next_state, done))\n",
    "\n",
    "    # pick samples from prioritized replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "\n",
    "        mini_batch, idxs, is_weights = self.memory.sample(self.batch_size)\n",
    "        mini_batch = np.array(mini_batch).transpose()\n",
    "\n",
    "        states = np.vstack(mini_batch[0])\n",
    "        actions = list(mini_batch[1])\n",
    "        rewards = list(mini_batch[2])\n",
    "        next_states = np.vstack(mini_batch[3])\n",
    "        dones = mini_batch[4]\n",
    "\n",
    "        # bool to binary\n",
    "        dones = dones.astype(int)\n",
    "\n",
    "        # Q function of current state\n",
    "        states = torch.Tensor(states)\n",
    "        states = Variable(states).float()\n",
    "        pred = self.model(states)\n",
    "\n",
    "        # one-hot encoding\n",
    "        a = torch.LongTensor(actions).view(-1, 1)\n",
    "\n",
    "        one_hot_action = torch.FloatTensor(self.batch_size, self.action_size).zero_()\n",
    "        one_hot_action.scatter_(1, a, 1)\n",
    "\n",
    "        pred = torch.sum(pred.mul(Variable(one_hot_action)), dim=1)\n",
    "\n",
    "        # Q function of next state\n",
    "        next_states = torch.Tensor(next_states)\n",
    "        next_states = Variable(next_states).float()\n",
    "        next_pred = self.target_model(next_states).data\n",
    "\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Q Learning: get maximum Q value at s' from target model\n",
    "        target = rewards + (1 - dones) * self.discount_factor * next_pred.max(1)[0]\n",
    "        target = Variable(target)\n",
    "\n",
    "        errors = torch.abs(pred - target).data.numpy()\n",
    "\n",
    "        # update priority\n",
    "        for i in range(self.batch_size):\n",
    "            idx = idxs[i]\n",
    "            self.memory.update(idx, errors[i])\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # MSE Loss function\n",
    "        is_weights = torch.from_numpy(is_weights).type(torch.FloatTensor)\n",
    "        loss = F.mse_loss(is_weights* pred, is_weights* target)\n",
    "        #loss = F.mse_loss(pred, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # and train\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, maximum length of episode is 500\n",
    "    #env = gym.make('CartPole-v0')\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    state_size = env.observation_.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    model = DQN(state_size, action_size)\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size, 0.6, 0.0001)\n",
    "    #agent1 = DQNAgent(state_size, action_size, 0,  1.0)\n",
    "    scores, episodes = [], []\n",
    "    success = 0\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        state = env.reset()        \n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            #reward = reward if not done or score == 499 else -10\n",
    "            reward = reward \n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # every time step do the training\n",
    "            if agent.memory.tree.n_entries >= agent.train_start:\n",
    "                agent.train_model()\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                agent.update_target_model()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                #score = score if score == 500 else score + 10\n",
    "                print(state)\n",
    "                if np.absolute(state[0][0]-0.5) < 0.05:\n",
    "                    success+=1\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                #pylab.savefig(\"./save_graph/cartpole_dqn.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      agent.memory.tree.n_entries, \"  epsilon:\", agent.epsilon, \" beta\", agent.memory.beta)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                #if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                    #torch.save(agent.model, \"./save_model/cartpole_dqn\")\n",
    "                    #sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, maximum length of episode is 500\n",
    "    #env = gym.make('CartPole-v0')\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    model = DQN(state_size, action_size)\n",
    "\n",
    "    #agent = DQNAgent(state_size, action_size, 0.6, 0.4)\n",
    "    agent = DQNAgent(state_size, action_size, 0,  1.0)\n",
    "    scores1, episodes1 = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score1 = 0\n",
    "        \n",
    "        state = env.reset()        \n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            #reward = reward if not done or score == 499 else -10\n",
    "            reward = reward \n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # every time step do the training\n",
    "            if agent.memory.tree.n_entries >= agent.train_start:\n",
    "                agent.train_model()\n",
    "\n",
    "            score1 += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                agent.update_target_model()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                #score = score if score == 500 else score + 10\n",
    "                scores1.append(score1)\n",
    "                episodes1.append(e)\n",
    "                pylab.plot(episodes1, scores1, 'b', episodes, scores, 'r')\n",
    "                #pylab.savefig(\"./save_graph/cartpole_dqn.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score1, \"  memory length:\",\n",
    "                      agent.memory.tree.n_entries, \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                #if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                    #torch.save(agent.model, \"./save_model/cartpole_dqn\")\n",
    "                    #sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "prioritized_experiment_replay.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
